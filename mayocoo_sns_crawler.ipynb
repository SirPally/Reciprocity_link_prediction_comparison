{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import  BeautifulSoup\n",
    "\n",
    "from requests import get\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_header = {\n",
    "        \"Request_from\" : \"BRIAN PALLANGYO\",\n",
    "        \"Email\" : \"<pallangyobrian@yahoo.com>\",\n",
    "        \"Purpose\" : \"Academic Research Study\",\n",
    "        \"Research_title\" : \"Comparative study of page rank and hits algorithms for reciprocal link prediction in online social networks\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mayocoo_base_url = \"https://www.mayocoo.com/\"\n",
    "following = \"/following\"\n",
    "followers = \"/followers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mayocoo_user (user):\n",
    "    mayocoo_link = mayocoo_base_url + user\n",
    "    return mayocoo_link\n",
    "\n",
    "def mayocoo_user_follow (user):\n",
    "    mayocoo_follow_link = mayocoo_base_url + user + following\n",
    "    return mayocoo_follow_link\n",
    "\n",
    "def mayocoo_user_follower (user):\n",
    "    mayocoo_followers_link = twitter_base_url + user + followers\n",
    "    return mayocoo_followers_link\n",
    "\n",
    "def url_to_following (url_string):\n",
    "    mayocoo_following = url_string + following\n",
    "    return mayocoo_following\n",
    "\n",
    "def get_user_details(user_name_id):\n",
    "    user_name_profile_link = mayocoo_user(user_name_id)\n",
    "    user_name_following_link = mayocoo_user_follow(user_name_id)\n",
    "    user_name_follower_link = mayocoo_user_follower(user_name_id)\n",
    "    number_of_following\n",
    "    number_of_followers\n",
    "    number_of_posts\n",
    "    location\n",
    "    gender\n",
    "    date_of_birth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting Crawling Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Queue\n",
    "queue = deque([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Visited Users\n",
    "visited_user_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_adjacency_list = \"\"\n",
    "final_node_list = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter SEED NODE\n",
    "seed_node = \"https://www.mayocoo.com/justice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawling and entering found users in the queue\n",
    "def crawl(url):\n",
    "    visited_user_list.append(url)\n",
    "    global final_adjacency_list\n",
    "    global final_node_list\n",
    "    \n",
    "    if len(queue) > 100000: #limit queue length to 100000nodes\n",
    "        return\n",
    "             \n",
    "    mayocoo_link = url_to_following (url) #go to user following page\n",
    "    try:\n",
    "        html = urlopen(mayocoo_link)\n",
    "    except:\n",
    "        #RESUME \n",
    "        #get url from queue to visit\n",
    "        next_url = queue.popleft()\n",
    "        #Call the Crawl function\n",
    "        crawl(next_url)\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    except:\n",
    "        #RESUME \n",
    "        next_url = queue.popleft()\n",
    "        crawl(next_url)\n",
    "        \n",
    "    result = soup.find_all('span', {'class' : 'user-popover'})\n",
    "    \n",
    "    #GET USER ATTRIBUTES\n",
    "    source_node = soup.find('img', {'class': 'pointer timelineuserpropic'}).get(\"id\")[12:]\n",
    "    username = soup.find('a', {'class': 'pronameblack'}).get_text().strip()\n",
    "    user_details = soup.find('ul', {'class': 'list-group'})\n",
    "    \n",
    " \n",
    "    #Initialize empty string for user details to overcome errors when no detail\n",
    "    user_no_posts = \" \"\n",
    "    user_gender = \" \"\n",
    "    date_of_birth = \" \"\n",
    "    user_work = \" \"\n",
    "    studying = \" \"\n",
    "    country = \" \"\n",
    "    region = \" \"\n",
    "    \n",
    "    for user_detail in user_details.find_all('li'):\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-edit zmdi-hc-stack-1x'})) != []: #NUmber of POSTS\n",
    "            user_no_posts = user_details.find_all('li')[1].get_text().strip()[:-6]\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-male-female zmdi-hc-stack-1x'})) != []: #GENDER\n",
    "            user_gender = user_details.find_all('li')[3].get_text().strip()\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-calendar zmdi-hc-stack-1x'})) != []: #DOB\n",
    "            date_of_birth = user_detail.get_text().strip()\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-case zmdi-hc-stack-1x'})) != []: #WORKING\n",
    "            user_work = user_detail.get_text().strip()[11:]\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-book zmdi-hc-stack-1x'})) != []: #studying\n",
    "            studying = user_detail.get_text().strip()[12:]\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-globe zmdi-hc-stack-1x'})) != []: #COUNTRY\n",
    "            country = user_detail.get_text().strip()[27:].replace(',', '')\n",
    "        if (user_detail.find_all('i', {'class': 'zmdi zmdi-pin zmdi-hc-stack-1x'})) != []: #REGION\n",
    "            region = user_detail.get_text().strip()[11:].replace(',', '')\n",
    "    \n",
    "    \n",
    "    #VISIT THE USER NODE\n",
    "    current_node_attrib = source_node + \",\" + url + \",\" + username + \",\" + user_no_posts + \",\" + user_gender + \",\" + date_of_birth + \",\" + user_work + \",\" + studying + \",\" + country + \",\" + region + \", \"\n",
    "    #final_node_list = final_node_list + current_node_attrib \n",
    "    \n",
    "    with open(\"mayocoo_nodeattrib.txt\", \"a+\") as text_file:\n",
    "        print(f\"{current_node_attrib}\", file=text_file)\n",
    "        \n",
    "    current_adjacency_list = source_node\n",
    "\n",
    "    \n",
    "    #Get user details\n",
    "    \n",
    "    ''' print(link_text.get(\"data-id\"))\n",
    "        print(link_text.a.get(\"href\"))\n",
    "        print(link_text.a.get_text())\n",
    "        print(\"\\t\")\n",
    "    '''\n",
    "    \n",
    "    for link_text in result:\n",
    "        flag = 0\n",
    "        current_adjacency_list = current_adjacency_list + \",\" + str(link_text.get(\"data-id\"))\n",
    "        \n",
    "        #Get followed user url only\n",
    "        user_url = link_text.a.get(\"href\")\n",
    "        \n",
    "        #print (\"Following USER_URL : \", user_url)\n",
    "        \n",
    "        #Check if user url exist in the queue\n",
    "        for j in queue:\n",
    "            if j == user_url:\n",
    "                flag = 1\n",
    "                break #if user url exist in queue then exit\n",
    "                \n",
    "        #if user url is not in the queue then proceed with execution len(queue)<100000\n",
    "        if flag == 0:\n",
    "            if len(queue) > 100000:\n",
    "                return\n",
    "            if (visited_user_list.count(user_url)) == 0:\n",
    "                queue.append(user_url)\n",
    "        \n",
    "    #final_adjacency_list = final_adjacency_list + \"\\n\" + current_adjacency_list\n",
    "    with open(\"mayocoo_adjlist.txt\", \"a+\") as text_file:\n",
    "        print(f\"{current_adjacency_list}\", file=text_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #get url from queue to visit\n",
    "    next_url = queue.popleft()\n",
    "    #print (\"NEXT_URL : \", next_url)\n",
    "    \n",
    "    #Call the Crawl function\n",
    "    crawl(next_url)\n",
    "        \n",
    "crawl(seed_node)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (final_adjacency_list)\n",
    "print (final_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visited :  2\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of visited :  %d\" % len(visited_user_list))\n",
    "with open(\"visited_user_list.txt\", \"a+\") as visited_text:\n",
    "    for each_list in visited_user_list:\n",
    "        print (f\"{each_list}\", file=visited_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
